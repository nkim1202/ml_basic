{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class MNIST:\n",
    "    \n",
    "    def __init__(self, num_input, num_hidden, num_output, learning_rate = 1e-2):\n",
    "        \n",
    "        print(\"*Info: Processing MNIST testing ...\")\n",
    "                \n",
    "        # initializing weights and biases in hidden/output layers    \n",
    "        self.num_input   = num_input\n",
    "        self.num_hidden  = num_hidden\n",
    "        self.num_output  = num_output\n",
    "        \n",
    "        self.input_data  = []\n",
    "        self.target_data = []\n",
    "        \n",
    "        self.W2 = np.random.randn(self.num_input, self.num_hidden) / np.sqrt(self.num_input/2)\n",
    "        self.b2 = np.random.rand(self.num_hidden)              \n",
    "        self.W3 = np.random.randn(self.num_hidden, self.num_output) / np.sqrt(self.num_hidden/2)\n",
    "        self.b3 = np.random.rand(self.num_output)      \n",
    "        \n",
    "        # initializing leanring rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # handling numerical errors\n",
    "        self.delta = 1e-7 \n",
    "       \n",
    "    def train(self, input_data, target_data):\n",
    "        \n",
    "        self.input_data  = input_data\n",
    "        self.target_data = target_data\n",
    "        \n",
    "        f = lambda x : self.feed_forward()\n",
    "        self.W2 -= self.learning_rate * self.numerical_derivative(f, self.W2)\n",
    "        self.b2 -= self.learning_rate * self.numerical_derivative(f, self.b2)\n",
    "            \n",
    "        self.W3 -= self.learning_rate * self.numerical_derivative(f, self.W3)\n",
    "        self.b3 -= self.learning_rate * self.numerical_derivative(f, self.b3)\n",
    "   \n",
    "    def feed_forward(self):\n",
    "            \n",
    "        z2 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        y2 = self.sigmoid(z2)\n",
    "        z3 = np.dot(y2, self.W3) + self.b3\n",
    "        y  = self.sigmoid(z3)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum(self.target_data*np.log(y + self.delta) + (1-self.target_data)*np.log((1 - y) + self.delta))   \n",
    "    \n",
    "    def loss_val(self):\n",
    "    \n",
    "        z2 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        y2 = self.sigmoid(z2)\n",
    "        z3 = np.dot(y2, self.W3) + self.b3\n",
    "        y  = self.sigmoid(z3)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum(self.target_data*np.log(y + self.delta) + (1-self.target_data)*np.log((1 - y) + self.delta))  \n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \n",
    "        z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        y2 = self.sigmoid(z2)\n",
    "        z3 = np.dot(y2, self.W3) + self.b3\n",
    "        y  = self.sigmoid(z3)\n",
    "    \n",
    "        # predict number would be the highest number in the final list\n",
    "        predict_num = np.argmax(y)\n",
    "    \n",
    "        return predict_num\n",
    "    \n",
    "    def accuracy(self, input_data, target_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        \n",
    "        # list which contains (index, label, prediction) value\n",
    "        index_label_prediction_list = []\n",
    "        \n",
    "        # temp list which contains label and prediction in sequence\n",
    "        temp_list = []\n",
    "        \n",
    "        for index in range(len(input_data)):\n",
    "                        \n",
    "            label = int(target_data[index])\n",
    "                        \n",
    "            # normalize\n",
    "            data = (input_data[index, :] / 255.0 * 0.99) + 0.01\n",
    "      \n",
    "            predicted_num = self.predict(data)\n",
    "        \n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "                \n",
    "            else:\n",
    "                not_matched_list.append(index)\n",
    "                \n",
    "                temp_list.append(index)\n",
    "                temp_list.append(label)\n",
    "                temp_list.append(predicted_num)\n",
    "                \n",
    "                index_label_prediction_list.append(temp_list)\n",
    "                \n",
    "                temp_list = []\n",
    "                \n",
    "        print(\"       Current Accuracy = \", len(matched_list)/(len(input_data)) )\n",
    "        \n",
    "        return matched_list, not_matched_list, index_label_prediction_list\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "\n",
    "    def numerical_derivative(self, f, x):\n",
    "\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index        \n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + self.delta\n",
    "            fx1 = f(x) # f(x+delta)\n",
    "\n",
    "            x[idx] = tmp_val - self.delta \n",
    "            fx2 = f(x) # f(x-delta)\n",
    "            grad[idx] = (fx1 - fx2) / (2*self.delta)\n",
    "\n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "        \n",
    "        return grad  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Info: Reading training data with shape of (60000, 785)\n"
     ]
    }
   ],
   "source": [
    "# training data : 60000 x 785\n",
    "# 60000 lines with 785 columns in each line\n",
    "# col[0] is the answer which is 0-9\n",
    "# col[1:784] is data \n",
    "training_data = np.loadtxt('./mnist_train.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "print(\"*Info: Reading training data with shape of\", training_data.shape)\n",
    "\n",
    "# num of inputs, hidden layers and outputs\n",
    "# num of input  nodes : 784\n",
    "# num of hidden nodes : 8  --> this could be any number.\n",
    "# num of output nodes : 10\n",
    "\n",
    "# initializing weightes and biases\n",
    "# (60000x784)x(784x8)x(8x10) = (60000x10)\n",
    "# W2 is (784x8)\n",
    "# W3 is (8x10)\n",
    "# basically it's (784x10) matrix.\n",
    "# by adding a hidden layer with '8',\n",
    "# it would be (784x8)x(8x10).\n",
    "\n",
    "num_inputs    = training_data.shape[1] - 1\n",
    "num_hiddens   = 8      \n",
    "num_outputs   = 10\n",
    "learning_rate = 1e-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  32. 121. 121. 206. 254. 183.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.  46. 157. 241. 244. 253. 253. 253. 253. 253.\n",
      " 242.  12.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 229. 238. 253. 253. 253. 253. 253. 253. 253. 253.\n",
      " 254. 229.  73.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 254. 253. 253. 253. 154.  39.  39.  39.  47. 215.\n",
      " 254. 253. 159.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 219. 253. 253.  84.  11.   0.  11.  25.   4. 165.\n",
      " 254. 253. 159.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.  32.  66.  66.  11.   0.  40. 156. 211. 191. 253.\n",
      " 254. 253. 229. 145.  54.  14.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  47. 233. 253. 253. 253. 253.\n",
      " 254. 253. 253. 253. 253. 166.  34.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  68. 253. 253. 253. 253. 253.\n",
      " 254. 253. 253. 253. 253. 253. 238.  59.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  64. 250. 253. 252. 240. 169.\n",
      " 107. 107. 107. 219. 250. 253. 253. 170.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0. 117. 232. 107.   0.   0.\n",
      "   0.   0.   0.   0.  95. 218. 253. 241.  57.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.  54. 255. 255. 241.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.  54. 253. 253. 240.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  65.  35.   0.   0.\n",
      "   0.   0.   0.   0.   0.   6. 182. 253. 240.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  60. 199.  93.   0.   0.\n",
      "   0.   0.   0.   0.   0.  46. 241. 253. 244.  38.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  54. 240. 253.  93.   0.   0.\n",
      "   0.   0.   0.   0.   0.  54. 253. 253. 250.  88.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  96. 253. 253. 122.   0.   0.\n",
      "   0.   0.   0.   0.  43.  97. 253. 253. 240.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 110. 253. 253. 245. 174. 174.\n",
      "  41.  41.  90. 174. 237. 253. 253. 253. 240.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  25. 192. 245. 253. 253. 253.\n",
      " 255. 253. 253. 253. 253. 253. 253. 236. 143.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  73. 191. 240. 253.\n",
      " 255. 253. 253. 253. 253. 250. 218.  39.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   7. 120.\n",
      " 183. 253. 253. 253. 147.  95.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.] [0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.13423531 0.4797647\n",
      " 0.4797647  0.80976474 0.99611765 0.7204706  0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.18858825\n",
      " 0.6195294  0.94564706 0.9572941  0.9922353  0.9922353  0.9922353\n",
      " 0.9922353  0.9922353  0.9495294  0.05658824 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.8990588  0.934      0.9922353  0.9922353\n",
      " 0.9922353  0.9922353  0.9922353  0.9922353  0.9922353  0.9922353\n",
      " 0.99611765 0.8990588  0.29341176 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.99611765 0.9922353  0.9922353  0.9922353  0.6078824  0.16141178\n",
      " 0.16141178 0.16141178 0.1924706  0.8447059  0.99611765 0.9922353\n",
      " 0.6272941  0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.8602353  0.9922353\n",
      " 0.9922353  0.33611766 0.05270588 0.01       0.05270588 0.10705882\n",
      " 0.02552941 0.6505883  0.99611765 0.9922353  0.6272941  0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.13423531 0.2662353  0.2662353  0.05270588\n",
      " 0.01       0.16529413 0.6156471  0.8291765  0.7515294  0.9922353\n",
      " 0.99611765 0.9922353  0.8990588  0.5729412  0.21964706 0.06435294\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.1924706  0.9145882\n",
      " 0.9922353  0.9922353  0.9922353  0.9922353  0.99611765 0.9922353\n",
      " 0.9922353  0.9922353  0.9922353  0.6544706  0.14200002 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.27400002 0.9922353  0.9922353  0.9922353\n",
      " 0.9922353  0.9922353  0.99611765 0.9922353  0.9922353  0.9922353\n",
      " 0.9922353  0.9922353  0.934      0.23905884 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.2584706  0.98058826 0.9922353  0.98835295 0.9417647  0.66611767\n",
      " 0.42541176 0.42541176 0.42541176 0.8602353  0.98058826 0.9922353\n",
      " 0.9922353  0.67       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.4642353\n",
      " 0.91070586 0.42541176 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.37882352 0.8563529  0.9922353  0.94564706\n",
      " 0.23129413 0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.21964706 1.         1.         0.94564706 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.21964706\n",
      " 0.9922353  0.9922353  0.9417647  0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.26235294 0.14588237 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.03329412 0.71658826 0.9922353\n",
      " 0.9417647  0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.24294119 0.78258824 0.37105882\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.18858825 0.94564706 0.9922353  0.9572941  0.15752943\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.21964706 0.9417647  0.9922353  0.37105882 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.21964706\n",
      " 0.9922353  0.9922353  0.98058826 0.35164705 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.3827059  0.9922353\n",
      " 0.9922353  0.48364705 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.17694119 0.38658825 0.9922353  0.9922353\n",
      " 0.9417647  0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.43705884 0.9922353  0.9922353  0.96117646\n",
      " 0.6855294  0.6855294  0.16917649 0.16917649 0.35941178 0.6855294\n",
      " 0.93011767 0.9922353  0.9922353  0.9922353  0.9417647  0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.10705882 0.7554118  0.96117646 0.9922353  0.9922353  0.9922353\n",
      " 1.         0.9922353  0.9922353  0.9922353  0.9922353  0.9922353\n",
      " 0.9922353  0.9262353  0.5651765  0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.29341176 0.7515294  0.9417647  0.9922353  1.         0.9922353\n",
      " 0.9922353  0.9922353  0.9922353  0.98058826 0.8563529  0.16141178\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.03717647 0.47588235 0.7204706  0.9922353  0.9922353  0.9922353\n",
      " 0.5807059  0.37882352 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01      ]\n",
      "3.0 [0.01 0.01 0.01 0.99 0.01 0.01 0.01 0.01 0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "# test training process.\n",
    "\n",
    "index         = np.random.randint(0, 59999)\n",
    "\n",
    "# normalizing input_data\n",
    "# input_data is 1x784 matrix. each one ranges 0 - 255 which\n",
    "# normalizes between 0 and 1 \n",
    "input_data = ((training_data[index, 1:] / 255.0) * 0.99) + 0.01\n",
    "print(training_data[index, 1:], input_data)\n",
    "\n",
    "# normalizing target_data\n",
    "# if answer is 9, then target_data[9] = 0.99 while others = 0.01\n",
    "#7.0 [0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.99 0.01 0.01]\n",
    "target_data = np.zeros(num_outputs) + 0.01    \n",
    "target_data[int(training_data[index, 0])] = 0.99\n",
    "        \n",
    "print(training_data[index, 0], target_data) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Info: Starting neural network learning using numerical derivative...\n",
      "*Info: Processing MNIST testing ...\n",
      "       epochs = 0 index = 0 loss = 6.215174990605565\n",
      "       epochs = 0 index = 200 loss = 3.3825424979876284\n",
      "       epochs = 0 index = 400 loss = 3.080731693840619\n",
      "       epochs = 0 index = 600 loss = 3.0942331082376198\n",
      "       epochs = 0 index = 800 loss = 3.246262665766574\n",
      "       epochs = 0 index = 1000 loss = 2.7356147580295525\n",
      "       epochs = 0 index = 1200 loss = 2.3938351915116387\n",
      "       epochs = 0 index = 1400 loss = 2.6312685478554045\n",
      "       epochs = 0 index = 1600 loss = 2.8080123370649686\n",
      "       epochs = 0 index = 1800 loss = 2.9555217533102085\n",
      "       epochs = 0 index = 2000 loss = 3.4024204719935347\n",
      "       epochs = 0 index = 2200 loss = 3.219151499579796\n",
      "       epochs = 0 index = 2400 loss = 2.5250405558708833\n",
      "       epochs = 0 index = 2600 loss = 3.432782967297394\n",
      "       epochs = 0 index = 2800 loss = 2.647006092629606\n",
      "       epochs = 0 index = 3000 loss = 2.92200932823048\n",
      "       epochs = 0 index = 3200 loss = 2.9796961220099036\n",
      "       epochs = 0 index = 3400 loss = 2.826327789407222\n",
      "       epochs = 0 index = 3600 loss = 2.355724670562278\n",
      "       epochs = 0 index = 3800 loss = 1.559503540539845\n",
      "       epochs = 0 index = 4000 loss = 2.1165634108608877\n",
      "       epochs = 0 index = 4200 loss = 1.970150135842467\n",
      "       epochs = 0 index = 4400 loss = 2.4607860809641044\n",
      "       epochs = 0 index = 4600 loss = 3.075584431640096\n",
      "       epochs = 0 index = 4800 loss = 2.18360435499385\n",
      "       epochs = 0 index = 5000 loss = 1.718258242282351\n",
      "       epochs = 0 index = 5200 loss = 2.255577429509531\n",
      "       epochs = 0 index = 5400 loss = 2.925566345293984\n",
      "       epochs = 0 index = 5600 loss = 2.4082209675908066\n",
      "       epochs = 0 index = 5800 loss = 2.1183419233153153\n",
      "       epochs = 0 index = 6000 loss = 1.4100859526248604\n",
      "       epochs = 0 index = 6200 loss = 2.4531939099524225\n",
      "       epochs = 0 index = 6400 loss = 1.2653687011657562\n",
      "       epochs = 0 index = 6600 loss = 2.786133970911231\n",
      "       epochs = 0 index = 6800 loss = 1.4666907919779626\n",
      "       epochs = 0 index = 7000 loss = 1.7528878311363731\n",
      "       epochs = 0 index = 7200 loss = 2.559572727629845\n",
      "       epochs = 0 index = 7400 loss = 2.1565763009684664\n",
      "       epochs = 0 index = 7600 loss = 2.473837643991738\n",
      "       epochs = 0 index = 7800 loss = 2.4953120619563975\n",
      "       epochs = 0 index = 8000 loss = 0.9731462239643671\n",
      "       epochs = 0 index = 8200 loss = 4.318833486051667\n",
      "       epochs = 0 index = 8400 loss = 1.6863841440297915\n",
      "       epochs = 0 index = 8600 loss = 2.4862730747552777\n",
      "       epochs = 0 index = 8800 loss = 1.6650676159535531\n",
      "       epochs = 0 index = 9000 loss = 1.760999049678991\n",
      "       epochs = 0 index = 9200 loss = 1.0828449530354185\n",
      "       epochs = 0 index = 9400 loss = 1.478201618163839\n",
      "       epochs = 0 index = 9600 loss = 1.2034391421194979\n",
      "       epochs = 0 index = 9800 loss = 2.3120310184421666\n",
      "       epochs = 0 index = 10000 loss = 1.411405267524916\n",
      "       epochs = 0 index = 10200 loss = 1.8329919499345106\n",
      "       epochs = 0 index = 10400 loss = 2.1251439883468115\n",
      "       epochs = 0 index = 10600 loss = 1.087800339268083\n",
      "       epochs = 0 index = 10800 loss = 2.0560342436578853\n",
      "       epochs = 0 index = 11000 loss = 0.9735675556776152\n",
      "       epochs = 0 index = 11200 loss = 0.8756258152698702\n",
      "       epochs = 0 index = 11400 loss = 0.929803650111334\n",
      "       epochs = 0 index = 11600 loss = 5.164057660307805\n",
      "       epochs = 0 index = 11800 loss = 1.1327335386711084\n",
      "       epochs = 0 index = 12000 loss = 1.2594566380195666\n",
      "       epochs = 0 index = 12200 loss = 1.2342979552919582\n",
      "       epochs = 0 index = 12400 loss = 1.1309085705156336\n"
     ]
    }
   ],
   "source": [
    "print(\"*Info: Starting neural network learning using numerical derivative...\")\n",
    "obj = MNIST(num_inputs, num_hiddens, num_outputs, learning_rate)\n",
    "\n",
    "epochs        = 1\n",
    "loss_val_list = []\n",
    "num_data      = len(training_data)\n",
    "start_time    = datetime.now()\n",
    "for step in range(epochs):\n",
    "    \n",
    "    for index in range(num_data):\n",
    "                \n",
    "        # normalizing input_data and target_data    \n",
    "        input_data = ((training_data[index, 1:] / 255.0) * 0.99) + 0.01\n",
    "        target_data = np.zeros(num_outputs) + 0.01    \n",
    "        target_data[int(training_data[index, 0])] = 0.99\n",
    "        \n",
    "        obj.train(input_data, target_data)\n",
    "        \n",
    "        if (index % 200 == 0):\n",
    "            print(\"       epochs = {0} index = {1} loss = {2}\" \\\n",
    "                  .format(step, index, obj.loss_val()))\n",
    "            \n",
    "        loss_val_list.append(obj.loss_val())        \n",
    "\n",
    "end_time = datetime.now()\n",
    "        \n",
    "print()\n",
    "print(\"*Info: Done with neural network learning using numerical derivative.\")\n",
    "print(\"       Elapsed Time: {0}\" .format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
